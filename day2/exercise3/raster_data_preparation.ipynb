{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f3a50a-175e-46d6-af17-e59e2cf6fc9f",
   "metadata": {},
   "source": [
    "# Raster data preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf8331-f3db-4e47-b645-aece1650789f",
   "metadata": {},
   "source": [
    "In this exercise, the raster data for the classification excercises is prepared. For machine learning tasks, the input raster data must follow these rules.\n",
    "\n",
    "1) Data and labels images must have the same:\n",
    "* Coordinate system\n",
    "* Resolution / pixel size\n",
    "* Data origin (min x and y), so that one pixel in both files covers exactly the same area.\n",
    "\n",
    "2) Labels image must be a raster image of type Integer and 1 band. The class numbers start from 0 and grow to `number_of_classes - 1`. So for binary classification the labels image has values 0 and 1. For 4 class multiclass labels, values 0 to 3. If needed, vector labels need to be rasterized.\n",
    "\n",
    "3) Data image must be of type Float with values between 0 and 1 (sometimes -1 to 1), may have several bands, usually has.\n",
    "\n",
    "Here we prepare labels for all data image area, for pixel-wise machine learning, data could be prepared also for specific points/pixels only. See for example: [SYKE land use classification with LUCAS points](https://geohpc.readthedocs.io/en/latest/lessons/L3/03_LandCoverClassification_syke_Parallelization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a09ad-6b69-42f7-97af-885cae3595f8",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "\n",
    "#### Labels:\n",
    "Data sources: \n",
    "* Finnish Food Authority, [Agricultural parcels](https://www.paikkatietohakemisto.fi/geonetwork/srv/eng/catalog.search#/metadata/e4467d0b-51f9-41f2-bd7e-8ff48a2cb083). Data downloaded from [GeoPortti GeoCubes](https://vm0160.kaj.pouta.csc.fi/geocubes/). Agricultural parcels are originally vector data, but have been rasterized to GeoCubes.\n",
    "* LUKE, [MVMI forest inventory, site main class](http://urn.fi/urn:nbn:fi:fd-849cabd2-0ae5-3455-b6f3-a39086cce33c). Data downloaded from [Paituli](https://paituli.csc.fi/).\n",
    "* SYKE, [CORINE](https://www.paikkatietohakemisto.fi/geonetwork/srv/eng/catalog.search#/metadata/%7B0B4B2FAC-ADF1-43A1-A829-70F02BF0C0E5%7D). Data downloaded from [GeoPortti GeoCubes](https://vm0160.kaj.pouta.csc.fi/geocubes/).\n",
    "\n",
    "\n",
    "#### Data image:\n",
    "* FMI, [10-days Sentinel2 mosaic](https://ckan.ymparisto.fi/dataset/sentinel-2-satellite-image-mosaics-s2gm-sentinel-2-satelliittikuvamosaiikki-s2gm), original images from ESA.\n",
    "\n",
    "The advantage of useing GeoCubes is that there all data is pre-processed so that it fits the requirements mentioned above. \n",
    "\n",
    "All datasets used for this exercise have already same coordinate system (20m). The `stackstac` package takes care of harmonizing data origin and resolution (20m).\n",
    "\n",
    "For finding the files [Paituli STAC](https://paituli.csc.fi/stac.html) is used. For longer intro to STAC and its usege see [CSC STAC examples](https://github.com/csc-training/geocomputing/tree/master/python/STAC).\n",
    "\n",
    "## Data processing results\n",
    "\n",
    "The goal of this exercise is to have 8 raster files:\n",
    "* 4 labels files and 4 data files with different extents\n",
    "* Extents: deep learning training, shallow learnging training, validation, test.\n",
    "* All files:\n",
    "    * Coordinate system: Finnish ETRS-TM35FIN, EPSG:3067\n",
    "    * Resolution: 20m\n",
    "\n",
    "#### Labels\n",
    "\n",
    "Multiclass classification raster: \n",
    "* 1 - forest from forest inventory data\n",
    "* 2 - fields from agricultural parcels data\n",
    "* 3 - water from CORINE land cover data\n",
    "* 0 - everything else \n",
    "\n",
    "#### Data image\n",
    "\n",
    "* Sentinel2 mosaic, we include data from 2 different dates (May and July), to have more data values. Final dataset has 8 bands based on bands: 2, 3, 4 and 8 on dates: 2021-05-11 and 2021-07-21, reflection values scaled to [0 ... 1]. The bands source data is: \n",
    "     *  'b02' / '2021-05-11'\n",
    "     *  'b02' / '2021-07-21'\n",
    "     *  'b03' / '2021-05-11'\n",
    "     *  'b03' / '2021-07-21'\n",
    "     *  'b04' / '2021-05-11'\n",
    "     *  'b04' / '2021-07-21'\n",
    "     *  'b08' / '2021-05-11'\n",
    "     *  'b08' / '2021-07-21'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbf6b2-f626-45cc-80fc-d13753cb827c",
   "metadata": {},
   "source": [
    "## Data processing main steps\n",
    "* Set STAC specs, file names and bbox extents for different files\n",
    "* Connect to STAC catalog and define function to fetch data via STAC\n",
    "\n",
    "#### Labels\n",
    "\n",
    "1) Fetch original datasets via Paituli STAC and explore them.\n",
    "2) Create a new labels dataset and calculate class values based on the original datasets\n",
    "3) Save data to 4 files with different extents.\n",
    "\n",
    "#### Data image\n",
    "\n",
    "1) Fetch Sentinel-2 11-day mosaics via Paituli STAC, including only required bands\n",
    "3) Normalize the data values, the reflectance values are originally between 0 and 1, but for storage they have been multiplied with 10 000, so here the normalization is simple division with 10 000.\n",
    "4) Stack data from different dates just as bands.\n",
    "3) Save data to 4 files with different extents.\n",
    "\n",
    "This workbook requires at least 6Gb memory. 1 core is enough, but xarray could use more, if working with bigger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586e7db-e946-4dd9-9456-764c8b741355",
   "metadata": {},
   "source": [
    "## Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1aa76-0787-4773-86cb-077e5cdbbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Handling raster data\n",
    "import rioxarray\n",
    "import xarray\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import pandas as pd\n",
    "\n",
    "# STAC search and data fetching\n",
    "import pyproj\n",
    "import pystac_client\n",
    "import stackstac\n",
    "\n",
    "# Bbox creation for plotting the extents\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1431c84-ce8c-4b35-b17e-170d9352c059",
   "metadata": {},
   "source": [
    "Set STAC specs for all datasets: collcetion_id, time-period and names of assets.\n",
    "\n",
    "Use [STAC browser](https://radiantearth.github.io/stac-browser/#/external/paituli.csc.fi/geoserver/ogc/stac/v1?.language=en) to find suitable datasets and their specs: \n",
    "* The collection ID is available behind I-source button.\n",
    "* Asset names, select a random item and see the left pane.\n",
    "* Available time, see Temporal Extent in STAC browser, use STAC search or look from GeoCubes or Paituli services, which years are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6eafd9-b5ca-4cbd-ac0d-8a5cca60ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paituli STAC\n",
    "stac_endpoint = \"https://paituli.csc.fi/geoserver/ogc/stac/v1\"\n",
    "\n",
    "# Fields \n",
    "fields_stac_id = \"land_parcels_at_geocubes\"\n",
    "fields_time = \"2016-07-01\"\n",
    "fields_assets = [\"20m\"]\n",
    "\n",
    "# CORINE\n",
    "corine_stac_id = \"corine_land_cover_at_geocubes\"\n",
    "corine_time = \"2018-07-01\"\n",
    "corine_assets = [\"20m\"]\n",
    "\n",
    "# Forest\n",
    "forest_stac_id = \"luke_vmi_paatyyppi_at_paituli\" \n",
    "forest_time = \"2023-07-01\"\n",
    "forest_assets = [\"luke_vmi_paatyyppi_at_paituli_tiff\"]\n",
    "\n",
    "# Sentinel-2 11-day mosaic\n",
    "sentinel2_stac_id = \"sentinel_2_11_days_mosaics_at_fmi\"\n",
    "sentinel2_time1 = \"2021-05-11\"\n",
    "sentinel2_time2 = \"2021-07-21\"\n",
    "sentinel2_assets = ['b02', 'b03', 'b04', 'b08']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbc058-0637-4c4d-a2fd-9e22c041e360",
   "metadata": {},
   "source": [
    "Set folder and file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb477b-de33-45fc-b0be-036a42e3d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders\n",
    "base_folder = os.path.join(os.sep, 'scratch', 'project_2000599', os.environ.get('USER'), 'lumi-aif-fmi', 'day2', 'exercise3')\n",
    "exercise_folder = os.path.join(base_folder) \n",
    "data_folder = os.path.join(base_folder,'data', 'raster')\n",
    "\n",
    "data_shallow = os.path.join(data_folder, 'data_shallow.tif')\n",
    "data_deep = os.path.join(data_folder, 'data_deep.tif')\n",
    "data_test = os.path.join(data_folder, 'data_test.tif')\n",
    "data_validation = os.path.join(data_folder, 'data_validation.tif')\n",
    "\n",
    "labels_shallow = os.path.join(data_folder, 'labels_shallow.tif')\n",
    "labels_deep = os.path.join(data_folder, 'labels_deep.tif')\n",
    "labels_test = os.path.join(data_folder, 'labels_test.tif')\n",
    "labels_validation = os.path.join(data_folder, 'labels_validation.tif')\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac1953-6e33-4218-a919-baa6640f5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir(exercise_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d868c38-3ef0-46a9-b557-67e7208774bd",
   "metadata": {},
   "source": [
    "Create new folder for prepared data, if it does not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56edec-7e0b-451e-a04b-6b5690e1fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(data_folder):\n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee2925-780e-485c-982c-d43928d1340c",
   "metadata": {},
   "source": [
    "### Set extent for different files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab800dc-ed45-4eea-9ef5-693d14ac5c6e",
   "metadata": {},
   "source": [
    "Define extents for training, validation and test data. The data is from Southern Finland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b271d36-a3b1-4893-824b-2620949f2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min_3067 = 250000\n",
    "y_min_3067 = 6690000\n",
    "x_max_3067 = x_min_3067 + 200000 \n",
    "y_max_3067 = y_min_3067 + 60000\n",
    "bbox_3067 = (x_min_3067, y_min_3067, x_max_3067, y_max_3067)\n",
    "\n",
    "x_min_3067_test = x_min_3067\n",
    "x_max_3067_test = x_min_3067_test + 20000\n",
    "x_min_3067_deep = x_max_3067_test\n",
    "x_max_3067_deep = x_max_3067 - 40000\n",
    "x_min_3067_shallow = x_min_3067_deep \n",
    "x_max_3067_shallow = x_min_3067_shallow + 20000\n",
    "x_min_3067_validation = x_max_3067_deep\n",
    "x_max_3067_validation = x_max_3067\n",
    "y_min_3067_shallow = y_min_3067 \n",
    "y_max_3067_shallow = y_min_3067 + 20000\n",
    "\n",
    "data_epsg = 3067\n",
    "data_epsg_string = \"EPSG:\" + str(data_epsg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a2497-9d58-4673-98bd-9b72e18b0897",
   "metadata": {},
   "source": [
    "Plot the different extents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c93d1-f5ad-450f-aab9-3d8cb6479f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_point_list_shallow = [x_min_3067_shallow, x_min_3067_shallow, x_max_3067_shallow, x_max_3067_shallow, x_min_3067_shallow]\n",
    "x_point_list_deep = [x_min_3067_deep, x_min_3067_deep, x_max_3067_deep, x_max_3067_deep, x_min_3067_deep]\n",
    "x_point_list_validation = [x_min_3067_validation, x_min_3067_validation, x_max_3067_validation, x_max_3067_validation, x_min_3067_validation]\n",
    "x_point_list_test = [x_min_3067_test, x_min_3067_test, x_max_3067_test, x_max_3067_test, x_min_3067_test]\n",
    "y_point_list = [y_min_3067, y_max_3067, y_max_3067, y_min_3067, y_min_3067]\n",
    "y_point_list_shallow = [y_min_3067_shallow, y_max_3067_shallow, y_max_3067_shallow, y_min_3067_shallow, y_min_3067_shallow]\n",
    "\n",
    "polygon_shallow = Polygon(zip(x_point_list_shallow, y_point_list_shallow))\n",
    "polygon_deep = Polygon(zip(x_point_list_deep, y_point_list))\n",
    "polygon_test = Polygon(zip(x_point_list_test, y_point_list))\n",
    "polygon_validation = Polygon(zip(x_point_list_validation, y_point_list))\n",
    "\n",
    "#polygons = [polygon_test, polygon_shallow, polygon_deep]\n",
    "d = {'label': ['deep', 'shallow', 'test', 'validation'], 'geometry': [polygon_deep, polygon_shallow, polygon_test, polygon_validation]}\n",
    "polygons = gpd.GeoDataFrame(d, crs=data_epsg_string)  \n",
    "polygons.explore(column='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce34a6-5cdf-415a-85a3-4b6e2f2475a4",
   "metadata": {},
   "source": [
    "## STAC set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478d779-33d5-4c6d-b6d5-7a773b6b3b0a",
   "metadata": {},
   "source": [
    "Open STAC end-point, the queries are done to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105449b5-361a-4f2b-89fd-b009c8f23117",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(stac_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616789a4-5f78-473e-b522-be3867f55547",
   "metadata": {},
   "source": [
    "Function to convert bbox to WGS-84 coordinate system, required by STAC search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c00ada-3154-46f6-bdab-8566f143e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_to_wgs(bbox, epsg_code):\n",
    "    x_min, y_min, x_max, y_max = bbox    \n",
    "    epsg_string = \"EPSG:\" + str(epsg_code)\n",
    "    long_min, lat_min = pyproj.Proj(epsg_string)(x_min, y_min, inverse=True)\n",
    "    long_max, lat_max  = pyproj.Proj(epsg_string)(x_max, y_max, inverse=True)\n",
    "    bbox_wgs = [long_min, lat_min, long_max, lat_max]\n",
    "    return bbox_wgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a603c9-85a4-4b65-ae0f-c37fdeca2c56",
   "metadata": {},
   "source": [
    "Function to query STAC catalog with given collection ID, time period and bbox. Fetch the data of found items, for given assets. Resolution (20m) is fixed here, but could be different from original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69caaf66-41f9-4ed2-9fc5-af3adf84290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stac_layer_with_bbox(collection_id, assets, time_period, bbox, epsg):\n",
    "\n",
    "    bbox_wgs = bbox_to_wgs(bbox, epsg)\n",
    "    #data_epsg_string = \"EPSG:\" + str(epsg)\n",
    "    \n",
    "    search = catalog.search(\n",
    "        bbox=bbox_wgs,\n",
    "        collections=[collection_id],\n",
    "        datetime=time_period,\n",
    "    )\n",
    "    \n",
    "    cube = stackstac.stack(\n",
    "        items=search.item_collection(),\n",
    "        bounds=bbox_3067, \n",
    "        assets=assets,\n",
    "        resolution=20,\n",
    "        xy_coords='center', \n",
    "        epsg=epsg\n",
    "    ).squeeze()\n",
    "    if \"time\" in cube.dims:\n",
    "        cube = cube.max(dim='time') # Here we use only data with one timestamp, but geocubes data is split to mapsheets, so this joins the mapsheets to same timestamp.\n",
    "    cube.compute()\n",
    "    return cube\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd3a07-cfc1-4242-9a26-ca55f5e673c1",
   "metadata": {},
   "source": [
    "## Labels processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135e87c-13f2-410c-90d0-6668fa53ce5a",
   "metadata": {},
   "source": [
    "Fetch data via STAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ba010-e47f-45bc-8163-645e59c2e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = fetch_stac_layer_with_bbox(forest_stac_id, forest_assets, forest_time, bbox_3067, data_epsg)\n",
    "forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f20fd5-d5d5-471c-9993-e00b31214116",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = fetch_stac_layer_with_bbox(fields_stac_id, fields_assets, fields_time, bbox_3067, data_epsg)\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd03802-a125-4288-b440-9ff82fb62a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "corine = fetch_stac_layer_with_bbox(corine_stac_id, corine_assets, corine_time, bbox_3067, data_epsg)\n",
    "corine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b06d6-afac-4d90-932c-9bb95d6e64bc",
   "metadata": {},
   "source": [
    "### Explore original labels data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21f84c-1865-441b-a3e7-00a84d8c64ec",
   "metadata": {},
   "source": [
    "Plot small part of the data image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091691c0-8269-4ac5-a98c-64bebfa5fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(15, 4))\n",
    "corine.isel(x=slice(1000,2000), y=slice(2000,3000)).plot(ax=ax[0])\n",
    "ax[0].set_title(\"Corine Land Cover\")\n",
    "fields.isel(x=slice(1000,2000), y=slice(2000,3000)).plot(ax=ax[1])\n",
    "ax[1].set_title(\"Fields\")\n",
    "forest.isel(x=slice(1000,2000), y=slice(2000,3000)).plot(ax=ax[2])\n",
    "ax[2].set_title(\"Forest main type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e996c0a-2f3f-4467-af72-ab8f56e437d3",
   "metadata": {},
   "source": [
    "### Reclassify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f77cb-21b8-4113-ae0c-d16b6396c50f",
   "metadata": {},
   "source": [
    "Create a new dataset with classification assigned based on the 3 original rasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954812e-51da-4cdf-9c97-8c080ea3ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = fields.copy()\n",
    "#labels = labels.where(labels != 2, other=0)\n",
    "labels = xarray.where(labels == 2, 0, labels) # Not-field to 0\n",
    "labels = xarray.where(labels == 1, 2, labels) # Fields to class 2\n",
    "labels = xarray.where(forest < 50, 1, labels) # Forest to class 1\n",
    "labels = xarray.where(corine == 47, 3, labels) # Rivers, to class 3\n",
    "labels = xarray.where(corine == 48, 3, labels) # Lakes, to class 3\n",
    "labels = xarray.where(corine == 49, 3, labels) # Sea, to class 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686c994-7ff6-4ea0-8f55-015e0df3d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3]\n",
    "colors  = [\"gray\", \"forestgreen\", \"lightyellow\", \"lightblue\"]\n",
    "cmap = ListedColormap(colors)\n",
    "class_bounds = np.array(classes + [classes[-1] + 1])   # e.g., [11,12,21,22,23,31,32]\n",
    "norm = BoundaryNorm(class_bounds, len(colors))\n",
    "labels.isel(x=slice(1000,2000), y=slice(2000,3000)).plot(cmap=cmap, norm=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b9142-e24a-4ac7-93bb-9f843c0c4cdc",
   "metadata": {},
   "source": [
    "See number of pixels in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f79d6-5e7b-4c55-bafc-afee0d0ee2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_stats(data):\n",
    "    stats = np.unique(data, return_counts=True)\n",
    "    df = pd.DataFrame({\n",
    "        \"value\": stats[0],\n",
    "        \"count\": stats[1]\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb15c0-d8e0-48b0-9143-37914fe798da",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_stats = get_class_stats(labels)\n",
    "print(\"Labels classes:\")\n",
    "labels_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0efe5-4d6a-4ecd-b69c-260d77f9b470",
   "metadata": {},
   "source": [
    "### Save labels files\n",
    "\n",
    "Add coordinate system info, as `rioxarray` wants it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901934fe-2514-4469-aff0-1ee21f039842",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.rio.write_crs(data_epsg, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d65cd8-135e-40bf-836f-6a81e78e8402",
   "metadata": {},
   "source": [
    "Save the file with 4 different extents for deep and shallow model training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569d5ea-22cd-4549-8536-813338e0373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bbox_to_geotiff(data, minx, miny, maxx, maxy, filename):\n",
    "    clipped = data.rio.clip_box(minx, miny, maxx, maxy)\n",
    "    clipped.rio.to_raster(filename, tiled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34c0c6-f782-413e-97b9-55595e889787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning, model training\n",
    "save_bbox_to_geotiff(labels, x_min_3067_deep, y_min_3067, x_max_3067_deep, y_max_3067, labels_deep)\n",
    "# Shallow learning, model training\n",
    "save_bbox_to_geotiff(labels, x_min_3067_shallow, y_min_3067_shallow, x_max_3067_shallow, y_max_3067_shallow, labels_shallow)\n",
    "# Validation\n",
    "save_bbox_to_geotiff(labels, x_min_3067_validation, y_min_3067, x_max_3067_validation, y_max_3067, labels_validation)\n",
    "# Test\n",
    "save_bbox_to_geotiff(labels, x_min_3067_test, y_min_3067, x_max_3067_test, y_max_3067, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d4d4f-6359-4836-aa5a-c5259414f181",
   "metadata": {},
   "source": [
    "## Data image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e06c7-a1f4-46a2-be59-828e2c4891f3",
   "metadata": {},
   "source": [
    "1. Find data using Paituli STAC catalogue, separately for 2 dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c69e2-00d5-4364-836a-cc0d56dc6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel2_1 = fetch_stac_layer_with_bbox(sentinel2_stac_id, sentinel2_assets, sentinel2_time1, bbox_3067, data_epsg)\n",
    "sentinel2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8757d1-5ad8-424a-9dcc-68207f6c89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel2_2 = fetch_stac_layer_with_bbox(sentinel2_stac_id, sentinel2_assets, sentinel2_time2, bbox_3067, data_epsg)\n",
    "sentinel2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c4f7a-330e-4946-a6d8-453520059481",
   "metadata": {},
   "source": [
    "Join data from both dates to one Xarray DataArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0efa2-57d8-4803-ab4e-320bca9b4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel2 = xarray.concat([sentinel2_1, sentinel2_2], dim=\"time\")\n",
    "sentinel2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a3c53-1909-427a-a9e9-4ec4fc523505",
   "metadata": {},
   "source": [
    "Normalize the data values, the reflectance values are originally between 0 and 1, but for storage Sentinel-2 data has been multiplied with 10 000, so here the normalization is simple division with 10 000. For other data sources, this step is likely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b9808-d463-4d92-acac-4266d570417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel2 = sentinel2 / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c02322-08a5-47de-a59e-1d281944cc5d",
   "metadata": {},
   "source": [
    "Plot small part of both scens as RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8cb038-e7d3-4afb-976b-14b136ccdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_2021_rgb = sentinel2.isel(x=slice(1000,2000), y=slice(2000,3000)).sel(band=['b04', 'b03', 'b02'])\n",
    "cube_2021_rgb.plot.imshow(row=\"time\", rgb=\"band\", robust=True, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2897f-f8c5-463a-9903-7ae04ba960c4",
   "metadata": {},
   "source": [
    "Stack different data from different dates just as bands for saving it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d4cbd-5b7e-47d9-9c5c-9a9d88a39b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = sentinel2.stack(z=(\"band\", \"time\"))\n",
    "stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071443ad-c7aa-4e43-beab-93971bdace3d",
   "metadata": {},
   "source": [
    "Change the axis order for rasterio and add coordinate system info as rasterio wants to have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8965c-1521-4493-804a-47e0b46795f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = stacked.transpose('z', 'y', 'x')\n",
    "final.rio.write_crs(\"epsg:3067\", inplace=True)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632d2a-4bef-4809-bf42-d7b9b0c400a2",
   "metadata": {},
   "source": [
    "Save image data to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ac815-1a78-4794-b89e-b7967760193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning, model training\n",
    "save_bbox_to_geotiff(final, x_min_3067_deep, y_min_3067, x_max_3067_deep, y_max_3067, data_deep)\n",
    "# Shallow learning, model training\n",
    "save_bbox_to_geotiff(final, x_min_3067_shallow, y_min_3067_shallow, x_max_3067_shallow, y_max_3067_shallow, data_shallow)\n",
    "# Validation\n",
    "save_bbox_to_geotiff(final, x_min_3067_validation, y_min_3067, x_max_3067_validation, y_max_3067, data_validation)\n",
    "# Test\n",
    "save_bbox_to_geotiff(final, x_min_3067_test, y_min_3067, x_max_3067_test, y_max_3067, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd57a47-3065-4366-8499-6cd78fd78166",
   "metadata": {},
   "source": [
    "Double-check that all is fine. Pay attention to coordinate system, pixel size and data origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9754ad-e2d3-4b3c-8ba6-b5fe47217252",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalinfo {data_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95ea01-31cf-400a-8d1d-e0c5505e3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalinfo {labels_test} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c94e3d-c37e-426e-8914-8e5bb50ba5eb",
   "metadata": {},
   "source": [
    "The data preparations for classification exercises are now ready, check `../data/raster` folder, that you have 8 .tif files there. Therea are also 8 .tif.aux.xml, which GDAL creates automatically with some metadata of the raster files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0be85-68af-446c-8793-fd5432055f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoml25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
