{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Land segmentation \n",
    "\n",
    "This notebooks:\n",
    "* Predicts land cover classes from test image using the trained UNET or ResNet model. \n",
    "    * For prediction the test data is tiled.\n",
    "* Computes the accuracy and IoU of the predictions by comparing the predictions to the ground truth.\n",
    "* Plots the results.\n",
    "\n",
    "Used labels:\n",
    "* 1 - forest from forest inventory data\n",
    "* 2 - fields from agricultural parcels data\n",
    "* 3 - water from CORINE land cover data\n",
    "* 0 - everything else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math, time\n",
    "from typing import Optional, Any, Tuple\n",
    "\n",
    "# Reading and writing raster data\n",
    "import rasterio\n",
    "\n",
    "# Torchgeo model\n",
    "import torch\n",
    "\n",
    "# Model evaluation\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassJaccardIndex, MulticlassF1Score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# Models\n",
    "from UNET_model import UNET\n",
    "from resnet_model import ResNet\n",
    "\n",
    "# fix torch.device()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define folders and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders\n",
    "# Set path to data and labels files\n",
    "base_folder = os.path.join(os.sep, 'scratch', 'project_2017263') \n",
    "data_folder = os.path.join(base_folder,'data', 'raster')\n",
    "\n",
    "data_test = os.path.join(data_folder, 'data_test.tif')\n",
    "labels_test = os.path.join(data_folder, 'labels_test.tif')\n",
    "output_folder = os.path.join(base_folder, os.environ.get('USER'), 'lumi-aif-fmi', 'day2', 'exercise3','inference')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "test_output = os.path.join(output_folder, 'segmentation_results.tif') \n",
    "test_output_all_classes = os.path.join(output_folder, 'segmentation_results_all_classes.tif') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "tile_size = 512 # Use the same as for model training, must be smaller than data height/width.\n",
    "batch_size = 8\n",
    "overlap = 20\n",
    "no_of_bands = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set computing device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model from checkpoint. Choose here either the trained ResNet or UNET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(8,4)\n",
    "model_name = model.__class__.__name__\n",
    "model.load_state_dict(torch.load(f'model_training/{model_name}_model.pt', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiled inference to predict the classes\n",
    "\n",
    "During inference the model should be given similar tiles as during model training, so again the big raster has to be tiled. The prediction quality on tile edges is often weak, so therefore we use overlapping tiles and use predictions the model is more confident of.\n",
    "\n",
    "The steps of tiled inference:\n",
    "* Calculate importances for each pixel in the tile, the pixels on the edge get lower importance, because usually there the model makes more mistakes.\n",
    "* Tile the raster into overlapping tiles.\n",
    "* Run inference on each tile. Each pixel gets probability value for each class - how likely this pixel belongs to any of the classes.\n",
    "    * Inference is practically run in batches, because so the GPU can be better utilized and the total time of prediction is smaller. \n",
    "* Merge the tiles, keep the estimation with highest probability, counting also with importance (distance to tile edge).\n",
    "\n",
    "One could also use SAHI for automatic inference with RGB images but as we have 6 input channels, we will write the script ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance matrix for each predicted tile\n",
    "\n",
    "Calculate importances for each pixel in the tile, the pixels on the edge get lower importance, because usually there the model makes more mistakes. Pixels in the center of the tile have higher importance. This helps with smooth blending at boundaries. Practically only pixels that overlap get reduced importance. In the plot, the darker the pixel, the less importance it has. \n",
    "\n",
    "Code modified from: https://github.com/opengeos/geoai/blob/main/geoai/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = tile_size\n",
    "w = tile_size\n",
    "\n",
    "y_grid, x_grid = np.mgrid[0:h, 0:w]\n",
    "\n",
    "# Calculate distance from each edge\n",
    "dist_from_left = x_grid\n",
    "dist_from_right = w - x_grid - 1\n",
    "dist_from_top = y_grid\n",
    "dist_from_bottom = h - y_grid - 1\n",
    "\n",
    "# Combine distances (minimum distance to any edge)\n",
    "edge_distance = np.minimum.reduce(\n",
    "    [\n",
    "        dist_from_left,\n",
    "        dist_from_right,\n",
    "        dist_from_top,\n",
    "        dist_from_bottom,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Convert to weight (higher weight for center pixels)\n",
    "# Normalize to [0, 1]\n",
    "edge_distance = np.minimum(edge_distance + 0.1, overlap / 2)\n",
    "importance = edge_distance / (overlap / 2)\n",
    "\n",
    "# Set same importances to all bands\n",
    "importances = torch.from_numpy(np.repeat(importance[np.newaxis, :, :], num_classes, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate prediction for a big raster.\n",
    "\n",
    "Code partly from: https://github.com/opengeos/geoai/blob/main/geoai/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_on_geotiff(\n",
    "    model: torch.nn.Module,\n",
    "    data,\n",
    "    tile_size: int = 512,\n",
    "    overlap: int = 256,\n",
    "    batch_size: int = 4,\n",
    "    num_channels: int = 3,\n",
    "    device: [torch.device] = \"cpu\",\n",
    "    **kwargs: Any,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform inference on a large GeoTIFF using a sliding window approach with improved blending.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model for inference.\n",
    "        data (numpy Array): Data of a GeoTIFF file.\n",
    "        tile_size (int): Size of sliding window for inference.\n",
    "        overlap (int): Overlap between adjacent tiles.\n",
    "        batch_size (int): Batch size for inference.\n",
    "        num_channels (int): Number of channels to use from the input image.\n",
    "        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n",
    "        **kwargs: Additional arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing output path and inference time in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    height = data.shape[1]\n",
    "    width = data.shape[2]\n",
    "\n",
    "    # Initialize predictions array with very small numbers\n",
    "    pixel_predictions = torch.full((num_classes, height, width), -float('inf'), device=device)\n",
    "\n",
    "    # Calculate the number of windows needed to cover the entire image\n",
    "    steps_y = math.floor((height - overlap) / (tile_size - overlap))\n",
    "    steps_x = math.floor((width - overlap) / (tile_size - overlap))\n",
    "\n",
    "    # Ensure we cover the entire image\n",
    "    last_y = height - tile_size\n",
    "    last_x = width - tile_size\n",
    "\n",
    "    total_windows = steps_y * steps_x\n",
    "    print(\n",
    "        f\"Processing {steps_y * steps_x} tiles with size {tile_size}x{tile_size} and overlap {overlap}...\"\n",
    "    )\n",
    "\n",
    "    # Process in batches, the calculation goes faster, if data is fed to GPU in batches.\n",
    "    batch_inputs = []\n",
    "    batch_positions = []\n",
    "    batch_count = 0\n",
    "\n",
    "    # Change data type to Float as required by the model.\n",
    "    image = data.astype(np.float32) \n",
    "\n",
    "    # Convert to tensor\n",
    "    image_tensor = torch.tensor(image, device=device)\n",
    "\n",
    "    # Slide window over the image - make sure we cover the entire image\n",
    "    for i in range(steps_y + 1):  # +1 to ensure we reach the edge\n",
    "        y = i * (tile_size - overlap)\n",
    "        y = min(i * (tile_size - overlap), last_y)\n",
    "\n",
    "        for j in range(steps_x + 1):  # +1 to ensure we reach the edge\n",
    "            x = j * (tile_size - overlap)\n",
    "            x = min(j * (tile_size - overlap), last_x)\n",
    "\n",
    "            # Add to batch\n",
    "            batch_inputs.append(image_tensor[:, y:y+tile_size, x:x+tile_size])\n",
    "\n",
    "            # Keep track where each tile is located\n",
    "            batch_positions.append((y, x))\n",
    "            batch_count += 1\n",
    "\n",
    "            # Process batch when it reaches the batch size or at the end\n",
    "            if batch_count == batch_size or (i == steps_y and j == steps_x):\n",
    "                batch_inputs_tensor = torch.stack(batch_inputs)\n",
    "                # Forward pass, give model a batch of data.\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(batch_inputs_tensor)\n",
    "\n",
    "                # Process each output in the batch.\n",
    "                for idx, output in enumerate(outputs):\n",
    "                    y_pos, x_pos, = batch_positions[idx]\n",
    "                    # Multiply with the importances based on pixel's distance to tile edge.\n",
    "                    weighted_scores = output * importances\n",
    "                    # Save predictions for the pixels/classes that have heigher score than previously saved.\n",
    "                    pixel_predictions[:, y_pos:y_pos+tile_size, x_pos:x_pos+tile_size] = torch.max(pixel_predictions[:, y_pos:y_pos+tile_size, x_pos:x_pos+tile_size], weighted_scores)\n",
    "\n",
    "                # Reset batch\n",
    "                batch_inputs = []\n",
    "                batch_positions = []\n",
    "                batch_count = 0\n",
    "    \n",
    "    # Calculate most probable class for each pixel\n",
    "    class_predictions = pixel_predictions.argmax(dim=0).numpy().astype(np.uint8)\n",
    "    \n",
    "    return pixel_predictions, class_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data from file and calculate predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(data_test) as src:\n",
    "    data = src.read()\n",
    "    pixel_predictions, class_predictions = inference_on_geotiff(model, data, tile_size, overlap, batch_size, no_of_bands, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results\n",
    "\n",
    "Calculate accuracy, intersection of union (IoU) and f1 as TorchGeo Multiclass metrics\n",
    "* Accuracy calculates how many pixels are correctly labeled\n",
    "* Iou measures the per-class overlap with predicted and true pixels\n",
    "* F1 calculates the harmonic mean between precision and recall for each class \n",
    "\n",
    "Results are averaged over all classes with 'macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open ground truth data for evaluation\n",
    "with rasterio.open(labels_test) as src:\n",
    "    ground_truth = src.read(1)\n",
    "\n",
    "# Convert both datasets to Pytorch tensors\n",
    "gt = torch.from_numpy(ground_truth)\n",
    "prediction_tensor = torch.from_numpy(class_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = MulticlassAccuracy(num_classes=num_classes, average='macro')\n",
    "iou = MulticlassJaccardIndex(num_classes=num_classes, average='macro')\n",
    "f1  = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "\n",
    "# calculate metrics using predicted mask and ground truth both with shape (B, H, W)\n",
    "accuracy = acc(prediction_tensor, gt)\n",
    "iou_val = iou(prediction_tensor, gt)\n",
    "f1_val = f1(prediction_tensor, gt)\n",
    "\n",
    "# print pixel accuracy, mean IoU and mean f1\n",
    "print({\n",
    "    \"Pixel Accuracy\": accuracy.item(),\n",
    "    \"Mean IoU\": iou_val.item(),\n",
    "    \"Mean F1\": f1_val.item()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification report: \\n', classification_report(ground_truth.reshape(-1), class_predictions.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ConfusionMatrixDisplay.from_predictions(ground_truth.reshape(-1), class_predictions.reshape(-1), normalize='true', cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure_.savefig(f\"{output_folder}/{model_name}_confusion_matrix_.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results: input data raster, predicted classes and ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better plotting of Sentinel image, normalize the values\n",
    "# Help function to normalize band values and enhance contrast. Just like what QGIS does automatically\n",
    "def normalize(array):\n",
    "    min_percent = 2   # Low percentile\n",
    "    max_percent = 98  # High percentile\n",
    "    lo, hi = np.percentile(array, (min_percent, max_percent), axis=(0,1), keepdims=True)\n",
    "    new_min, new_max = 1, 255\n",
    "    rgb_norm = (rgb - lo) / (hi - lo) * (new_max - new_min) + new_min\n",
    "    rgb_norm = rgb_norm.astype(np.uint8)    \n",
    "    return rgb_norm.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open raster\n",
    "with rasterio.open(data_test) as src:\n",
    "    # Get RGB channels of May data\n",
    "    rgb = src.read([5, 3, 1]) #\n",
    "    #transpose to get H,W,C\n",
    "    rgb = np.transpose(rgb, (1, 2, 0))\n",
    "    rgb_norm = normalize(rgb)\n",
    "\n",
    "    # Plot the 3 rasters\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 20))\n",
    "\n",
    "    # Set the colors for classified data\n",
    "    cmap = ListedColormap([\"black\", \"forestgreen\", \"lightyellow\", \"lightblue\"])\n",
    "\n",
    "    # Sentinel data\n",
    "    axes[0].imshow(rgb_norm)\n",
    "    axes[0].set_title(\"Input image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Predicted classes\n",
    "    axes[1].imshow(class_predictions, cmap=cmap)\n",
    "    axes[1].set_title(\"Predicted classes\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Ground truth\n",
    "    axes[2].imshow(ground_truth, cmap=cmap)\n",
    "    axes[2].set_title(\"Ground truth\")\n",
    "    axes[2].axis(\"off\")\n",
    "      \n",
    "    plt.savefig(f\"{output_folder}/{model_name}_segmentation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that other class has some roads actually visible, although they are not predicted as `other` with default settings. If needed, we could make `other` class probabilities manually higher to get `other` class better represented in the final classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoml25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
